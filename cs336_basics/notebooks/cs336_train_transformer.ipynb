{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyMSGdT1uX1b2xlSV7m0Trb+"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4GHh1-5Tq4y",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c1658aba-71cb-4621-ff75-91fda0ccd752"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "✓ Copied training data: TinyStoriesV2-GPT4-train.npy\n",
      "✓ Copied validation data: TinyStoriesV2-GPT4-valid.npy\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setup directories\n",
    "drive_cs336_dir = \"/content/drive/MyDrive/Colab/cs336\"\n",
    "\n",
    "# Copy tokenizer files from Drive to local\n",
    "def setup_tokenizer_files():\n",
    "   \"\"\"Copy tokenizer files from Google Drive to local directory\"\"\"\n",
    "   vocab_source = f\"{drive_cs336_dir}/tinystories_vocab.json\"\n",
    "   merges_source = f\"{drive_cs336_dir}/tinystories_merges.txt\"\n",
    "\n",
    "   vocab_dest = \"tinystories_vocab.json\"\n",
    "   merges_dest = \"tinystories_merges.txt\"\n",
    "\n",
    "   if os.path.exists(vocab_source):\n",
    "       shutil.copy2(vocab_source, vocab_dest)\n",
    "\n",
    "   if os.path.exists(merges_source):\n",
    "       shutil.copy2(merges_source, merges_dest)\n",
    "\n",
    "# Copy encoded data files from Drive to local\n",
    "def setup_data_files():\n",
    "   \"\"\"Copy encoded data files from Google Drive to local directory\"\"\"\n",
    "   train_source = f\"{drive_cs336_dir}/TinyStoriesV2-GPT4-train.npy\"\n",
    "   valid_source = f\"{drive_cs336_dir}/TinyStoriesV2-GPT4-valid.npy\"\n",
    "\n",
    "   train_dest = \"TinyStoriesV2-GPT4-train.npy\"\n",
    "   valid_dest = \"TinyStoriesV2-GPT4-valid.npy\"\n",
    "\n",
    "   if os.path.exists(train_source):\n",
    "       shutil.copy2(train_source, train_dest)\n",
    "       print(f\"✓ Copied training data: {train_dest}\")\n",
    "   else:\n",
    "       print(f\"❌ Training data not found: {train_source}\")\n",
    "\n",
    "   if os.path.exists(valid_source):\n",
    "       shutil.copy2(valid_source, valid_dest)\n",
    "       print(f\"✓ Copied validation data: {valid_dest}\")\n",
    "   else:\n",
    "       print(f\"❌ Validation data not found: {valid_source}\")\n",
    "\n",
    "# Setup all files\n",
    "setup_tokenizer_files()\n",
    "setup_data_files()\n",
    "\n",
    "# Now you can use the files locally:\n",
    "# tokenizer = Tokenizer.from_files(\"tinystories_vocab.json\", \"tinystories_merges.txt\", [\"<|endoftext|>\"])\n",
    "# train_data = np.load(\"TinyStoriesV2-GPT4-train.npy\")\n",
    "# valid_data = np.load(\"TinyStoriesV2-GPT4-valid.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install jaxtyping\n",
    "!pip install wandb -qU"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTaD2xnoVdnH",
    "outputId": "676c9d23-66da-402d-8854-a49a9edfc014"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting jaxtyping\n",
      "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
      "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: wadler-lindig, jaxtyping\n",
      "Successfully installed jaxtyping-0.3.2 wadler-lindig-0.1.7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def pretokenize_for_encoding(text, special_tokens=None):\n",
    "    \"\"\"Tokenize text into a list of byte tuples for encoding.\"\"\"\n",
    "    pattern = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    if not special_tokens:\n",
    "        # Fast path for no special tokens\n",
    "        matches = list(pattern.finditer(text))\n",
    "        result = []\n",
    "        for match in matches:\n",
    "            token_bytes = tuple(match.group(0).encode(\"utf-8\"))\n",
    "            result.append(token_bytes)\n",
    "        return result\n",
    "\n",
    "    # Sort special tokens by length (longest first) to handle overlapping tokens correctly\n",
    "    special_tokens_sorted = sorted(special_tokens, key=len, reverse=True)\n",
    "    special_tokens_set = set(special_tokens)\n",
    "\n",
    "    # Create pattern that matches longest tokens first\n",
    "    special_pattern = '|'.join(re.escape(token) for token in special_tokens_sorted)\n",
    "\n",
    "    # Split text on special tokens\n",
    "    text_segments = re.split(f'({special_pattern})', text)\n",
    "\n",
    "    result = []\n",
    "    for segment in text_segments:\n",
    "        if not segment:\n",
    "            continue\n",
    "        if segment in special_tokens_set:\n",
    "            result.append(tuple(segment.encode(\"utf-8\")))\n",
    "        else:\n",
    "            # Process regular text segments\n",
    "            matches = list(pattern.finditer(segment))\n",
    "            for match in matches:\n",
    "                token_bytes = tuple(match.group(0).encode(\"utf-8\"))\n",
    "                result.append(token_bytes)\n",
    "\n",
    "    return result\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]], special_tokens: list[str] | None = None):\n",
    "        self.vocab = vocab\n",
    "        self._rvocab = {v: k for k, v in vocab.items()}\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        # Create merge lookup table for O(1) access\n",
    "        self.merge_lookup = {}\n",
    "        for i, (left, right) in enumerate(merges):\n",
    "            pair = (left, right)\n",
    "            self.merge_lookup[pair] = i\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath: str, merge_filepath:str, special_tokens: list[str] = [\"<|endoftext|>\"]) -> \"Tokenizer\":\n",
    "        with open(vocab_filepath, \"r\", encoding='utf-8') as f:\n",
    "            vocab_data = json.load(f)\n",
    "            vocab = {int(k): base64.b64decode(v.encode('utf-8')) for k, v in vocab_data.items()}\n",
    "\n",
    "        with open(merge_filepath, \"r\", encoding='utf-8') as f:\n",
    "            merges = []\n",
    "            for line in f:\n",
    "                parts = line.rstrip().split(\" \")\n",
    "                if len(parts) == 2:\n",
    "                    left = base64.b64decode(parts[0])\n",
    "                    right = base64.b64decode(parts[1])\n",
    "                    merges.append((left, right))\n",
    "        return Tokenizer(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        # Get pretokens (each is a tuple of bytes)\n",
    "        print(\"Pretokenizing...\")\n",
    "        pretokens = pretokenize_for_encoding(text, self.special_tokens)\n",
    "        all_tokens = []\n",
    "\n",
    "        # Add progress bar for processing pretokens\n",
    "        for pretoken in tqdm(pretokens, desc=\"Encoding pretokens\", unit=\"pretoken\"):\n",
    "            # Check if this pretoken is a special token\n",
    "            pretoken_bytes = bytes(pretoken)\n",
    "            pretoken_str = pretoken_bytes.decode('utf-8', errors='ignore')\n",
    "\n",
    "            if self.special_tokens and pretoken_str in self.special_tokens:\n",
    "                # Handle special token - look it up directly in vocab\n",
    "                if pretoken_bytes in self._rvocab:\n",
    "                    special_token_id = self._rvocab[pretoken_bytes]\n",
    "                    all_tokens.append(special_token_id)\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token '{pretoken_str}' not found in vocabulary\")\n",
    "            else:\n",
    "                # Handle regular token - convert to individual bytes first\n",
    "                tokens = []\n",
    "                for byte_val in pretoken:\n",
    "                    single_byte = bytes([byte_val])  # Convert int to bytes object\n",
    "                    if single_byte in self._rvocab:\n",
    "                        token_id = self._rvocab[single_byte]\n",
    "                        tokens.append(token_id)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Byte {single_byte} (ASCII {byte_val}) not found in vocabulary\")\n",
    "\n",
    "                # Apply merges using efficient algorithm\n",
    "                while True:\n",
    "                    # Find the earliest merge available\n",
    "                    earliest_merge = None  # (position, merge_index)\n",
    "\n",
    "                    for i in range(len(tokens) - 1):\n",
    "                        # Get the byte pair at position i\n",
    "                        left_bytes = self.vocab[tokens[i]]\n",
    "                        right_bytes = self.vocab[tokens[i + 1]]\n",
    "                        pair = (left_bytes, right_bytes)\n",
    "\n",
    "                        # Check if this pair has a merge rule\n",
    "                        merge_index = self.merge_lookup.get(pair, -1)\n",
    "                        if merge_index != -1:\n",
    "                            # If this is the earliest merge found so far, save it\n",
    "                            if earliest_merge is None or merge_index < earliest_merge[1]:\n",
    "                                earliest_merge = (i, merge_index)\n",
    "\n",
    "                    # If no merge found, we're done\n",
    "                    if earliest_merge is None:\n",
    "                        break\n",
    "\n",
    "                    # Apply the earliest merge\n",
    "                    pos, merge_idx = earliest_merge\n",
    "                    left_bytes = self.vocab[tokens[pos]]\n",
    "                    right_bytes = self.vocab[tokens[pos + 1]]\n",
    "                    merged_bytes = left_bytes + right_bytes\n",
    "\n",
    "                    if merged_bytes in self._rvocab:\n",
    "                        merged_token_id = self._rvocab[merged_bytes]\n",
    "                        # Replace the two tokens with the merged token\n",
    "                        tokens = tokens[:pos] + [merged_token_id] + tokens[pos + 2:]\n",
    "                    else:\n",
    "                        # This shouldn't happen if vocab is consistent with merges\n",
    "                        break\n",
    "\n",
    "                # Add processed tokens from this pretoken to final result\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        return all_tokens\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        decoded_bytes = b''.join(self.vocab[id] for id in ids)\n",
    "        return decoded_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for chunk in iterable:\n",
    "            # Use your existing pretokenize_for_encoding function\n",
    "            pretokens = pretokenize_for_encoding(chunk, self.special_tokens)\n",
    "\n",
    "            for pretoken in pretokens:\n",
    "                # Check if this pretoken is a special token\n",
    "                pretoken_bytes = bytes(pretoken)\n",
    "                pretoken_str = pretoken_bytes.decode('utf-8', errors='ignore')\n",
    "\n",
    "                if self.special_tokens and pretoken_str in self.special_tokens:\n",
    "                    # Handle special token - look it up directly in vocab\n",
    "                    if pretoken_bytes in self._rvocab:\n",
    "                        special_token_id = self._rvocab[pretoken_bytes]\n",
    "                        yield special_token_id\n",
    "                    else:\n",
    "                        raise ValueError(f\"Special token '{pretoken_str}' not found in vocabulary\")\n",
    "                    continue\n",
    "\n",
    "                # Handle regular token - convert to individual bytes first\n",
    "                tokens = []\n",
    "                for byte_val in pretoken:\n",
    "                    single_byte = bytes([byte_val])  # Convert int to bytes object\n",
    "                    if single_byte in self._rvocab:\n",
    "                        token_id = self._rvocab[single_byte]\n",
    "                        tokens.append(token_id)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Byte {single_byte} (ASCII {byte_val}) not found in vocabulary\")\n",
    "\n",
    "                # Apply merges to this pretoken\n",
    "                for left_bytes, right_bytes in self.merges:\n",
    "                    new_tokens = []\n",
    "                    i = 0\n",
    "                    while i < len(tokens):\n",
    "                        # Check if we can merge at position i\n",
    "                        if (i < len(tokens) - 1 and\n",
    "                            self.vocab[tokens[i]] == left_bytes and\n",
    "                            self.vocab[tokens[i + 1]] == right_bytes):\n",
    "                            # Merge: find token ID for merged bytes\n",
    "                            merged_bytes = left_bytes + right_bytes\n",
    "                            if merged_bytes in self._rvocab:\n",
    "                                merged_token_id = self._rvocab[merged_bytes]\n",
    "                                new_tokens.append(merged_token_id)\n",
    "                                i += 2  # Skip both tokens\n",
    "                            else:\n",
    "                                new_tokens.append(tokens[i])\n",
    "                                i += 1\n",
    "                        else:\n",
    "                            new_tokens.append(tokens[i])\n",
    "                            i += 1\n",
    "                    tokens = new_tokens\n",
    "\n",
    "                # Yield the processed tokens from this pretoken\n",
    "                for token_id in tokens:\n",
    "                    yield token_id"
   ],
   "metadata": {
    "id": "OFu3n8ZOZfQv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from jaxtyping import Float, Int\n",
    "import numpy.typing as npt\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features:int, out_features:int, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        W = torch.empty(out_features, in_features, device=device, dtype=dtype)\n",
    "        std = (2/(in_features + out_features)**(0.5))\n",
    "        torch.nn.init.trunc_normal_(W, mean=0, std=std, a=-3*std, b=3*std)\n",
    "        self.W = nn.Parameter(W)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return einsum(x, self.W, \"... d_in, d_out d_in -> ... d_out\")\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size:int, d_model:int, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        W = torch.empty(self.vocab_size, self.d_model, device=device, dtype=dtype)\n",
    "        torch.nn.init.trunc_normal_(W, mean=0, std=1, a=-3, b=3)\n",
    "        self.W = nn.Parameter(W)\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.W[token_ids]\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        # self.eps = eps\n",
    "        # self.G = nn.Parameter(torch.ones(d_model, device=device, dtype=dtype))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x\n",
    "        # in_dtype = x.dtype\n",
    "        # x = x.to(torch.float32)\n",
    "        # x_squared = x**2\n",
    "        # x_squared_mean = x_squared.mean(-1, keepdim=True)\n",
    "        # rms = (x_squared_mean + self.eps)**(0.5)\n",
    "        # x_normalized = x / rms\n",
    "        # result = einsum(x_normalized, self.G, \"... d_model, d_model -> ... d_model\")\n",
    "        # return result.to(in_dtype)\n",
    "\n",
    "class SWIGLU(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        self.W1 = Linear(d_model, d_ff, device, dtype)\n",
    "        self.W2 = Linear(d_ff, d_model, device, dtype)\n",
    "        self.W3 = Linear(d_model, d_ff, device, dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w1x = self.W1(x)\n",
    "        silux = w1x * torch.sigmoid(w1x)\n",
    "        w3x = self.W3(x)\n",
    "        elew1w3 = silux * w3x\n",
    "        return self.W2(elew1w3)\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
    "        super().__init__()\n",
    "        self.r = torch.zeros(max_seq_len, d_k, d_k, device=device)\n",
    "\n",
    "        for i in range(max_seq_len):\n",
    "            for k in range(d_k//2):\n",
    "                freq = 1.0 / (theta ** (2*k / d_k))\n",
    "                angle = i * freq\n",
    "\n",
    "                cos_val = torch.cos(torch.tensor(angle, device=device))\n",
    "                sin_val = torch.sin(torch.tensor(angle, device=device))\n",
    "\n",
    "                self.r[i, 2*k, 2*k] = cos_val\n",
    "                self.r[i, 2*k, 2*k+1] = -sin_val\n",
    "                self.r[i, 2*k+1, 2*k] = sin_val\n",
    "                self.r[i, 2*k+1, 2*k+1] = cos_val\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        ri_token_pos = self.r[token_positions]\n",
    "        return einsum(x, ri_token_pos, \"... seq d_k_in, ... seq d_k_out d_k_in -> ... seq d_k_out\")\n",
    "\n",
    "\n",
    "def softmax(x: torch.Tensor, dim: int):\n",
    "    max_xi = torch.amax(x, dim=dim, keepdim=True)\n",
    "    x_shifted = x - max_xi\n",
    "    x_exp = torch.exp(x_shifted)\n",
    "    sum_x_exp = torch.sum(x_exp, dim=dim, keepdim=True)\n",
    "    result = x_exp / sum_x_exp\n",
    "    return result\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    Q: Float[Tensor, \" ... queries d_k\"],\n",
    "    K: Float[Tensor, \" ... keys d_k\"],\n",
    "    V: Float[Tensor, \" ... values d_v\"],\n",
    "    mask: Float[Tensor, \" ... queries keys\"] | None = None\n",
    ") -> Float[Tensor, \" ... queries d_v\"]:\n",
    "    wei = einsum(Q, K, \"... queries d_k, ... keys d_k -> ... queries keys\") / (Q.shape[-1] ** 0.5)\n",
    "    if mask is not None:\n",
    "        mask = mask.to(wei.device)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "    wei = softmax(wei, dim=-1)\n",
    "    return einsum(wei, V, \"... queries keys, ... keys d_v -> ... queries d_v\")\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model:int, num_heads:int, device=None, rope=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.Q = Linear(d_model, self.dk * self.num_heads, device=device)\n",
    "        self.K = Linear(d_model, self.dk * self.num_heads, device=device)\n",
    "        self.V = Linear(d_model, self.dk * self.num_heads, device=device)\n",
    "        self.Wo = Linear(self.dk * num_heads, d_model, device=device)\n",
    "        self.rope = rope\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        q,k,v = self.Q(x), self.K(x), self.V(x)\n",
    "        q = rearrange(q, \"... seq (num_heads dk) -> ... num_heads seq dk\", num_heads=self.num_heads)\n",
    "        k = rearrange(k, \"... seq (num_heads dk) -> ... num_heads seq dk\", num_heads=self.num_heads)\n",
    "        v = rearrange(v, \"... seq (num_heads dv) -> ... num_heads seq dv\", num_heads=self.num_heads)\n",
    "\n",
    "        if self.rope != None:\n",
    "            q = self.rope(q, token_positions)\n",
    "            k = self.rope(k, token_positions)\n",
    "\n",
    "        seq = k.shape[-2]\n",
    "        attn = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        attn = rearrange(attn, \"... num_heads seq dv -> ... seq (num_heads dv)\")\n",
    "        return self.Wo(attn)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, device=None, rope=None):\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(d_model, num_heads, device=device, rope=rope)\n",
    "        self.ffn = SWIGLU(d_model, d_ff, device=device)\n",
    "        self.ln1 = RMSNorm(d_model, device=device)\n",
    "        self.ln2 = RMSNorm(d_model, device=device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        if token_positions == None:\n",
    "            token_positions = torch.arange(x.shape[1])\n",
    "        x = x + self.mha(self.ln1(x), token_positions)\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size:int, context_length:int, num_layers:int, d_model:int, num_heads:int, d_ff:int, rope_theta=None, device=None):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(vocab_size, d_model, device)\n",
    "        d_k = d_model // num_heads\n",
    "        self.rope = RotaryPositionalEmbedding(rope_theta, d_k, context_length, device)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(d_model, num_heads, d_ff, device, self.rope) for _ in range(num_layers)])\n",
    "        self.lnf = RMSNorm(d_model, device=device)\n",
    "        self.lm_head = Linear(d_model, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        tok_emb = self.emb(x)\n",
    "        block_out = self.blocks(tok_emb)\n",
    "        norm_out = self.lnf(block_out)\n",
    "        logits = self.lm_head(norm_out)\n",
    "        return logits"
   ],
   "metadata": {
    "id": "cVpiEH7xVECk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from collections.abc import Callable, Iterable\n",
    "from pathlib import Path\n",
    "from typing import Any, BinaryIO, IO, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import einsum, rearrange\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cross_entropy(inputs: Float[Tensor, \" batch_size vocab_size\"], targets: Int[Tensor, \" batch_size\"]) -> Float[Tensor, \"\"]:\n",
    "    inputs_shifted = inputs - torch.max(inputs, dim=-1, keepdim=True).values\n",
    "\n",
    "    log_sum_exp = torch.log(torch.sum(torch.exp(inputs_shifted), dim=-1, keepdim=True))\n",
    "\n",
    "    logits = inputs_shifted - log_sum_exp\n",
    "\n",
    "    nlls = -logits[torch.arange(len(targets)), targets]\n",
    "\n",
    "    return nlls.mean()\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        defaults = {\"lr\" : lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                t = state.get(\"t\", 0)\n",
    "                p.data -= lr / math.sqrt(t + 1) * p.grad.data\n",
    "                state[\"t\"] = t + 1\n",
    "        return loss\n",
    "\n",
    "class AdamW(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, weight_decay=0.0, betas=(0.9,0.999), eps = 1e-8):\n",
    "        defaults = {\"lr\" : lr, \"betas\": betas, \"weight_decay\": weight_decay, \"eps\": eps}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            b1,b2 = group[\"betas\"]\n",
    "            eps = group[\"eps\"]\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                t = state.get(\"t\", 1)\n",
    "                m = state.get(\"m\", torch.zeros_like(p.data))\n",
    "                v = state.get(\"v\", torch.zeros_like(p.data))\n",
    "                g = p.grad.data\n",
    "                m = b1 * m + (1-b1) * g\n",
    "                v = b2*v + (1-b2) * g**2\n",
    "                lr_t = lr * math.sqrt((1-b2**t)) / (1 - b1**t)\n",
    "                p.data -= lr_t * m / (torch.sqrt(v) + eps)\n",
    "                p.data -= lr * weight_decay * p.data\n",
    "                state[\"t\"] = t + 1\n",
    "                state[\"m\"] = m\n",
    "                state[\"v\"] = v\n",
    "        return loss\n",
    "\n",
    "def lr_cosine_schedule(\n",
    "    it: int,\n",
    "    max_lr: float,\n",
    "    min_lr: float,\n",
    "    warmup_steps: int,\n",
    "    cosine_cycle_iters: int,\n",
    "):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * it / warmup_steps\n",
    "    # 2) if it > cosine_cycle_iters, return min learning rate\n",
    "    if it >= cosine_cycle_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (cosine_cycle_iters - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "def gradient_clipping(parameters: Iterable[torch.nn.Parameter], max_l2_norm: float, eps=1e-6) -> None:\n",
    "    \"\"\"Given a set of parameters, clip their combined gradients to have l2 norm at most max_l2_norm.\n",
    "\n",
    "    Args:\n",
    "        parameters (Iterable[torch.nn.Parameter]): collection of trainable parameters.\n",
    "        max_l2_norm (float): a positive value containing the maximum l2-norm.\n",
    "\n",
    "    The gradients of the parameters (parameter.grad) should be modified in-place.\n",
    "    \"\"\"\n",
    "    grad_params = [p for p in parameters if p.grad is not None]\n",
    "    l2norm = torch.sqrt(sum([torch.sum(p.grad **2) for p in grad_params]))\n",
    "    if l2norm < max_l2_norm:\n",
    "        return l2norm\n",
    "    for p in grad_params:\n",
    "        p.grad *= (max_l2_norm/ (l2norm + eps))\n",
    "    return l2norm\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    dataset: npt.NDArray, batch_size: int, context_length: int, device: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Pre-allocate numpy arrays\n",
    "    xs = np.zeros((batch_size, context_length), dtype=np.int64)\n",
    "    ys = np.zeros((batch_size, context_length), dtype=np.int64)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        idx = np.random.randint(0, len(dataset) - context_length)\n",
    "        xs[i] = dataset[idx: idx + context_length]\n",
    "        ys[i] = dataset[idx+1: idx + context_length + 1]\n",
    "\n",
    "    # Convert to tensors\n",
    "    xs = torch.from_numpy(xs).to(device)\n",
    "    ys = torch.from_numpy(ys).to(device)\n",
    "    return (xs, ys)\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    iteration: int,\n",
    "    out: str | os.PathLike | BinaryIO | IO[bytes],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a model, optimizer, and an iteration number, serialize them to disk.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Serialize the state of this model.\n",
    "        optimizer (torch.optim.Optimizer): Serialize the state of this optimizer.\n",
    "        iteration (int): Serialize this value, which represents the number of training iterations\n",
    "            we've completed.\n",
    "        out (str | os.PathLike | BinaryIO | IO[bytes]): Path or file-like object to serialize the model, optimizer, and iteration to.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"iteration\": iteration\n",
    "    }\n",
    "    torch.save(checkpoint, out)\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    src: str | os.PathLike | BinaryIO | IO[bytes],\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a serialized checkpoint (path or file-like object), restore the\n",
    "    serialized state to the given model and optimizer.\n",
    "    Return the number of iterations that we previously serialized in\n",
    "    the checkpoint.\n",
    "\n",
    "    Args:\n",
    "        src (str | os.PathLike | BinaryIO | IO[bytes]): Path or file-like object to serialized checkpoint.\n",
    "        model (torch.nn.Module): Restore the state of this model.\n",
    "        optimizer (torch.optim.Optimizer): Restore the state of this optimizer.\n",
    "    Returns:\n",
    "        int: the previously-serialized number of iterations.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(src)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    return checkpoint[\"iteration\"]"
   ],
   "metadata": {
    "id": "2hxnNFH-Vj5B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "import random\n",
    "import math\n",
    "\n",
    "wandb.login()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "45lioJ092y17",
    "outputId": "c46e9c7b-381b-402a-8ea3-c891cef5a8db"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkl4kennylee81\u001b[0m (\u001b[33mkl4kennylee81-kenneth-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Literal, cast, Optional, BinaryIO, IO\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Default training configuration at top of file\n",
    "DefaultTrainModelArgs = {\n",
    "    # Model args\n",
    "    \"vocab_size\": 10000,\n",
    "    \"context_length\": 256,\n",
    "    \"num_layers\": 4,\n",
    "    \"d_model\": 512,\n",
    "    \"num_heads\": 16,\n",
    "    \"d_ff\": 1344,\n",
    "    \"rope_theta\": 10000,\n",
    "\n",
    "    # Optimizer args\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "\n",
    "    # Learning rate schedule\n",
    "    \"max_learning_rate\": 1e-3,\n",
    "    \"min_learning_rate\": 1e-5,\n",
    "    \"warmup_iters\": 2000,\n",
    "    \"cosine_cycle_iters\": 40960,\n",
    "\n",
    "    # Data paths - keep as is\n",
    "    \"training_set\": \"TinyStoriesV2-GPT4-train.npy\",\n",
    "    \"validation_set\": \"TinyStoriesV2-GPT4-valid.npy\",\n",
    "    \"tokenizer_vocab\": \"tinystories_vocab.json\",\n",
    "    \"tokenizer_merges\": \"tinystories_merges.txt\",\n",
    "\n",
    "    # Training config\n",
    "    \"validation_step_interval\": 500,\n",
    "    \"checkpoint_step_interval\": 10000,\n",
    "    \"steps\": 40960,  # 327M tokens target\n",
    "    \"batch_size\": 32,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "\n",
    "    # gdrive\n",
    "    \"save_gdrive\": False,\n",
    "    \"load_model_gdrive\": \"\",\n",
    "\n",
    "    # Device\n",
    "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "\n",
    "    # wandb\n",
    "    \"wandb_active\": False,\n",
    "    \"wandb_run\": \"\"\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class TrainModelArgs:\n",
    "    # model args\n",
    "    vocab_size: int = 10000\n",
    "    context_length: int = 256\n",
    "    num_layers: int = 4\n",
    "    d_model: int = 512\n",
    "    num_heads: int = 16\n",
    "    d_ff: int = 1344\n",
    "    rope_theta: Optional[int] = 10000\n",
    "\n",
    "    # adamw args\n",
    "    weight_decay: float = 0.01\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "\n",
    "    # Learning rate schedule\n",
    "    max_learning_rate: float = 1e-3\n",
    "    min_learning_rate: float = 1e-5\n",
    "    warmup_iters: int = 2000\n",
    "    cosine_cycle_iters: int = 40960\n",
    "\n",
    "    # training loop args\n",
    "    training_set: str | os.PathLike | BinaryIO | IO[bytes] = \"TinyStoriesV2-GPT4-train.npy\"\n",
    "    validation_set: str | os.PathLike | BinaryIO | IO[bytes] = \"TinyStoriesV2-GPT4-valid.npy\"\n",
    "    tokenizer_vocab: str | os.PathLike | BinaryIO | IO[bytes] = \"tinystories_vocab.json\"\n",
    "    tokenizer_merges: str | os.PathLike | BinaryIO | IO[bytes] = \"tinystories_merges.txt\"\n",
    "\n",
    "    validation_step_interval: int = 500\n",
    "    checkpoint_step_interval: int = 10000\n",
    "    steps: int = 40960\n",
    "    batch_size: int = 32\n",
    "    gradient_clipping: Optional[float] = 1.0\n",
    "\n",
    "    # gdrive\n",
    "    save_gdrive: bool = False\n",
    "    load_model_gdrive: str = \"\"\n",
    "\n",
    "    # wandb logging\n",
    "    wandb_active: bool = False\n",
    "    wandb_run: Optional[str] = \"\"\n",
    "\n",
    "    # device\n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self, args: TrainModelArgs):\n",
    "        self.args = args\n",
    "        self.cur_step = 0\n",
    "        self.model = Transformer(\n",
    "            vocab_size=args.vocab_size,\n",
    "            context_length=args.context_length,\n",
    "            num_layers=args.num_layers,\n",
    "            num_heads=args.num_heads,\n",
    "            d_model=args.d_model,\n",
    "            d_ff=args.d_ff,\n",
    "            rope_theta=args.rope_theta,\n",
    "            device=args.device\n",
    "        )\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.max_learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            betas=args.betas\n",
    "        )\n",
    "\n",
    "        self.tokenizer = Tokenizer.from_files(args.tokenizer_vocab, args.tokenizer_merges, [\"<|endoftext|>\"])\n",
    "\n",
    "        self.training_set = np.load(self.args.training_set, mmap_mode='r')\n",
    "        self.validation_set = np.load(self.args.validation_set, mmap_mode='r')\n",
    "\n",
    "        if args.wandb_active and wandb.run:\n",
    "            wandb.watch(self.model, log=cast(Literal[\"gradients\", \"parameters\", \"all\"], \"gradients\"), log_freq=10)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            total_size = self.training_set.size + self.validation_set.size\n",
    "            eval_size = total_size // 1000\n",
    "            num_batches = eval_size // (self.args.batch_size * self.args.context_length)\n",
    "\n",
    "            num_batches = max(1, num_batches)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                x, label = get_batch(self.validation_set, self.args.batch_size, self.args.context_length, device=self.args.device)\n",
    "                with torch.autocast(device_type=self.args.device, dtype=torch.bfloat16):\n",
    "                  output = self.model(x)\n",
    "                loss = cross_entropy(output, label)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = torch.tensor(total_loss / num_batches)\n",
    "            perplexity = avg_loss.exp()\n",
    "            return avg_loss, perplexity\n",
    "\n",
    "    def train(self):\n",
    "        if self.args.load_model_gdrive != \"\":\n",
    "          self.cur_step = load_checkpoint(self.args.load_model_gdrive, self.model, self.optimizer)\n",
    "\n",
    "        valid_loss, valid_perplexity = self.evaluate()\n",
    "        if self.args.wandb_active and wandb.run:\n",
    "            wandb.log({\"valid_loss\": valid_loss, \"valid_perplexity\": valid_perplexity}, step=self.cur_step)\n",
    "\n",
    "        pbar = tqdm(range(self.cur_step, self.args.steps))\n",
    "        start_time = time.time()\n",
    "        tokens_processed = 0\n",
    "\n",
    "        for step in pbar:\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            self.cur_step = step\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            lr = lr_cosine_schedule(\n",
    "                step,\n",
    "                self.args.max_learning_rate,\n",
    "                self.args.min_learning_rate,\n",
    "                self.args.warmup_iters,\n",
    "                self.args.cosine_cycle_iters)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            x, targets = get_batch(self.training_set, self.args.batch_size, self.args.context_length, device=self.args.device)\n",
    "            with torch.autocast(device_type=self.args.device, dtype=torch.bfloat16):\n",
    "              logits = self.model(x)\n",
    "            loss = cross_entropy(logits, targets)\n",
    "            loss.backward()\n",
    "            l2norm = gradient_clipping(self.model.parameters(), self.args.gradient_clipping)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Calculate metrics\n",
    "            batch_tokens = x.shape[0] * x.shape[1]\n",
    "            tokens_processed += batch_tokens\n",
    "            elapsed_time = time.time() - start_time\n",
    "            tokens_per_second = tokens_processed / elapsed_time if elapsed_time > 0 else 0\n",
    "            dt = time.time() - step_start_time\n",
    "\n",
    "            if self.args.save_gdrive and step % self.args.checkpoint_step_interval == 0 and step > 0 :\n",
    "                os.makedirs(f'{drive_cs336_dir}/output', exist_ok=True)\n",
    "                save_checkpoint(self.model, self.optimizer, step, f'{drive_cs336_dir}/output/checkpoint-{step}.pth')\n",
    "\n",
    "            if (step % self.args.validation_step_interval == 0 and step > 0) or (step == self.args.steps-1):\n",
    "                valid_loss, valid_perplexity = self.evaluate()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.2f}\",\n",
    "                \"valid_loss\": f\"{valid_loss.item():.2f}\",\n",
    "                \"valid_perplexity\": f\"{valid_perplexity.item():.2f}\",\n",
    "            })\n",
    "\n",
    "            if self.args.wandb_active and wandb.run:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": loss.item(),\n",
    "                    \"train_perplexity\": loss.exp().item(),\n",
    "                    \"valid_loss\": valid_loss.item(),\n",
    "                    \"valid_perplexity\": valid_perplexity.item(),\n",
    "                    \"grad_norm\": l2norm,\n",
    "                    \"lr\": lr,\n",
    "                    \"tokens_per_second\": tokens_per_second,\n",
    "                    \"step_time_seconds\": dt,\n",
    "                    \"gpu_memory_gb\": torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0,\n",
    "                    \"tokens_processed\": tokens_processed,\n",
    "                }, step=step)\n",
    "\n",
    "        # Save final checkpoint\n",
    "        if self.args.save_gdrive:\n",
    "          save_checkpoint(self.model, self.optimizer, step, f'{drive_cs336_dir}/output/checkpoint-{step}.pth')\n",
    "\n",
    "        if self.args.wandb_active and wandb.run:\n",
    "            local_checkpoint_path = f'{self.args.wandb_run}-checkpoint-{step}.pth'\n",
    "            save_checkpoint(self.model, self.optimizer, step, local_checkpoint_path)\n",
    "\n",
    "            artifact = wandb.Artifact(f\"{self.args.wandb_run}-checkpoint_{step}\", type=\"model\")\n",
    "            artifact.add_file(local_checkpoint_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "            # Clean up local file after uploading to wandb\n",
    "            os.remove(local_checkpoint_path)\n"
   ],
   "metadata": {
    "id": "ESqKGuiyUMBT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Complete sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'valid_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [32,64,128,256]},\n",
    "    }\n",
    "}\n",
    "\n",
    "def train_sweep():\n",
    "    # Initialize wandb run\n",
    "    run = wandb.init(\n",
    "      group=\"Tinystories-lr-sweep\",\n",
    "      force=True\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    # Create training arguments - only override what's different from defaults\n",
    "    train_args = {\n",
    "        **DefaultTrainModelArgs,\n",
    "\n",
    "        # Only the sweep parameters that differ from DefaultTrainModelArgs\n",
    "        \"steps\": 160000 // config.batch_size, # 40,960,000 tokens processed\n",
    "        \"batch_size\": config.batch_size,\n",
    "        \"validation_step_interval\": 4096 // config.batch_size,\n",
    "        \"cosine_cycle_iters\": 160000 // config.batch_size,\n",
    "        \"warmup_iters\": 8000 // config.batch_size,\n",
    "        \"max_learning_rate\": 1e-3 * (config.batch_size//16),\n",
    "        \"min_learning_rate\": 1e-5 * (config.batch_size//16),\n",
    "\n",
    "        # wandb settings - always override for sweep\n",
    "        \"wandb_active\": True,                     # Enable for sweep\n",
    "        \"wandb_run\" : f\"batch_size_{config.batch_size}\"\n",
    "    }\n",
    "\n",
    "    # Initialize and run training\n",
    "    trainer = TrainModel(TrainModelArgs(**train_args))\n",
    "    config.update(asdict(trainer.args))\n",
    "    wandb.run.name = trainer.args.wandb_run\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "\n",
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=sweep_config,\n",
    "    project=\"cs336-llm-assignment1\",\n",
    "    entity=\"kl4kennylee81-kenneth-personal\"\n",
    ")\n",
    "\n",
    "print(f\"Sweep created successfully!\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "print(f\"Project: cs336-llm-assignment1\")\n",
    "print(f\"wandb agent {sweep_id}\")\n",
    "\n",
    "# Run the sweep agent\n",
    "# wandb.agent(sweep_id, train_sweep, count=8)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1b4865fda6f54ed585f81b708b836972",
      "ce6b5caf779e493c856d40abf1f24cb9",
      "70ebca37e49e408aaa12f987b768a339",
      "a1fea5bb86e949a9b815dcbec978b3df",
      "3448619e57b64b5fb7db94aefde83f6f",
      "c1a5fe4c896341b09a33d283f4449b45",
      "17bbd0a68a8b4d9a906f9c24f2f5d1b4",
      "7b063159cd7748bcb5f103614c27fe32",
      "1f8b58b4e4d34718a0dfc93f851275cf",
      "c8b99a73f32947778e0e784a76eb71e0",
      "af3fe5a362784e2f9579c5092a8cff2b",
      "3f4f3227bf38474f81a64b89838e31ee",
      "052950cde97743268f83d9d0b5ef0539",
      "0709abda91e74cf4a79865f92f7b5f17",
      "e10ca68763d04fbeb43e45acab0e491f",
      "b7184dc5b36d41e2bd5b3c86ea0b8296",
      "4431891671b749a0a8610fc9210bb2ab",
      "f8af1dec965545e8b5524d1861e9c0f1",
      "64f3c7e80b3749498ef6f48af266bfa4",
      "b7ba5289ac2f4034bea498a463980a68",
      "8b89ceea3a2d4b46885ee183c3422c98",
      "0052f62666dd48b59e56f097595a1a99",
      "41e20424b7584ad2b73dee9fa14b4293",
      "8fce90dea47448d5a1bdc131ca3f8118",
      "1bf9de2c66eb4ad4a8068193929ebc3f",
      "5e19ae4e1b734f5b9c97a3bb54ce889d",
      "381b46f822bc4bcd903ac2004f285908",
      "3a64ff82aa754ce3a560a0977a8d2ad8",
      "d255debcb27b48738330de66494b8dc0",
      "ca5122a461144e6a9f130b539fb545e0",
      "90febe52dc2b41169946fcb623ad79e4",
      "aa3d3b6811824b8f81d893964bc94063",
      "71937040c8dd404a9ed4891d989f344b",
      "3385e67058be456f9cec952cc15b251e",
      "6bd04e89d4964e7a86c05fece9bb819b",
      "fd0f26b88c8d4aebbc93d1923e07543c",
      "c165b5a24f0848ec991f08a0b3155d46",
      "26d2d7086c294f28ae040870135d45e0",
      "cee3b325c3f14660b234d39c7e8cfaa0",
      "41ad060da8d54ec39e119be6a799dfd6",
      "7d448ebf0fd4483fa4deb48f4ba55380",
      "75b54f8a316b45e28024b757f84a459d",
      "9147ec008126458d94612384aae5a4c1",
      "1fcab70cef6d4918b34652d9cbd83d41",
      "22eb714894f4422fa2e695479a212ed9",
      "67707003961b473e95c106bf84fda394",
      "0d780f8bdb3e41eebf7117a3ee470265",
      "563d84c10a624ca0bdaa874d00710ede",
      "bcb93f34ecd2491cafdf563a07678712",
      "df20f90793634fe48753bff3594b8b18",
      "5dfb64b27e7a4b489cb8ab2bcc4fabde",
      "a90fa4f06b734c318713a3d5f9833957",
      "3cd7865fcc304d968c811302575b8b74",
      "811965a1808a4b1981a1ff193dcda559",
      "41ca672dcb554429a98767079bdffbb2"
     ]
    },
    "id": "DDk0vplIU9xi",
    "outputId": "f9aab907-ac4a-4970-8d79-90b62fe7ce2f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create sweep with ID: i8mzslpy\n",
      "Sweep URL: https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy\n",
      "Sweep created successfully!\n",
      "Sweep ID: i8mzslpy\n",
      "Project: cs336-llm-assignment1\n",
      "wandb agent i8mzslpy\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iwalby0f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250723_152419-iwalby0f</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/iwalby0f' target=\"_blank\">leafy-sweep-1</a></strong> to <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/iwalby0f' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/iwalby0f</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>▂▄▁▄▃▄▂▅▃▄▁▇▁▃▃█▂▅▃▂▃▃▂▃▅▄▃▁▄▂▄▄▄▂▅▂▁▄▂▃</td></tr><tr><td>grad_norm</td><td>█▂▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▅▆▇██████▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>step_time_seconds</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁█▁▁█▁▁▇▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>tokens_per_second</td><td>▁▁▄██▃▄▂▃▄▅▅▂▄▁▁▁▂▃▂▂▃▁▂▂▃▂▁▂▂▂▂▂▁▂▂▂▂▂▁</td></tr><tr><td>tokens_processed</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▄▄▄▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>█▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_perplexity</td><td>███▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>0.54792</td></tr><tr><td>grad_norm</td><td>0.41117</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>step_time_seconds</td><td>0.09436</td></tr><tr><td>tokens_per_second</td><td>52447.0584</td></tr><tr><td>tokens_processed</td><td>40960000</td></tr><tr><td>train_loss</td><td>1.76822</td></tr><tr><td>train_perplexity</td><td>5.86039</td></tr><tr><td>valid_loss</td><td>1.68932</td></tr><tr><td>valid_perplexity</td><td>5.41581</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size_16</strong> at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/iwalby0f' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/iwalby0f</a><br> View project at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250723_152419-iwalby0f/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1ms0miee with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250723_153744-1ms0miee</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/1ms0miee' target=\"_blank\">deep-sweep-2</a></strong> to <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/1ms0miee' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/1ms0miee</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>▂▂▄▃▄▄█▁▃▆▃▆▅▁▃▂▃▅▃▂▄▅▁▂▃▄▁▄▄▂▂▃▂▂▃▃▄▂▃▅</td></tr><tr><td>grad_norm</td><td>█▆▅▅▄▃▄▄▄▃▃▄▄▂▂▃▂▂▂▂▃▂▃▃▂▂▂▂▂▂▂▂▁▁▂▁▂▂▂▂</td></tr><tr><td>lr</td><td>▅▇█████████▇▇▆▆▆▆▅▅▅▅▅▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>step_time_seconds</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tokens_per_second</td><td>██▃▃▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tokens_processed</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇█▆▇▆▆▅▄▆▅▄▄▅▃▄▄▃▃▂▃▃▄▂▂▂▂▂▂▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>███▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>0.71196</td></tr><tr><td>grad_norm</td><td>0.29603</td></tr><tr><td>lr</td><td>2e-05</td></tr><tr><td>step_time_seconds</td><td>0.14097</td></tr><tr><td>tokens_per_second</td><td>61168.39582</td></tr><tr><td>tokens_processed</td><td>40960000</td></tr><tr><td>train_loss</td><td>1.87473</td></tr><tr><td>train_perplexity</td><td>6.51907</td></tr><tr><td>valid_loss</td><td>1.80881</td></tr><tr><td>valid_perplexity</td><td>6.10321</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size_32</strong> at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/1ms0miee' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/1ms0miee</a><br> View project at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250723_153744-1ms0miee/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0q84cjq0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250723_154915-0q84cjq0</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/0q84cjq0' target=\"_blank\">young-sweep-3</a></strong> to <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/0q84cjq0' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/0q84cjq0</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁█▁█▁▁▁▁█▁▁▁▁▁▁</td></tr><tr><td>grad_norm</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▃▃▂▃█▅▂▃▂▂▂▄▂▅▂▇▂▃▇▂▃█▃▂▂</td></tr><tr><td>lr</td><td>▆████████▇▇▇▇▇▇▆▅▅▅▄▄▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>step_time_seconds</td><td>▁▁▁▁▁█▁█▁▂██▁▁▁▇▁▁▂▂▂▁▂▇▁▂▁▂▁▁▁▁▂▂▁▁█▁▂█</td></tr><tr><td>tokens_per_second</td><td>█▂▂▃▃▂▃▂▂▂▁▂▂▂▁▂▂▂▁▁▁▂▁▂▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>tokens_processed</td><td>▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▇▅▅▆▅▆▄▅▆▇▅▆▃▄▃▄▄▃▄▂▃▃▃▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▁</td></tr><tr><td>valid_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_perplexity</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>1.04062</td></tr><tr><td>grad_norm</td><td>0.67908</td></tr><tr><td>lr</td><td>4e-05</td></tr><tr><td>step_time_seconds</td><td>0.23545</td></tr><tr><td>tokens_per_second</td><td>67020.07536</td></tr><tr><td>tokens_processed</td><td>40960000</td></tr><tr><td>train_loss</td><td>2.20698</td></tr><tr><td>train_perplexity</td><td>9.08824</td></tr><tr><td>valid_loss</td><td>2.26537</td></tr><tr><td>valid_perplexity</td><td>9.6347</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size_64</strong> at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/0q84cjq0' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/0q84cjq0</a><br> View project at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250723_154915-0q84cjq0/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fdwacwaa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "creating run (1.4s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250723_155943-fdwacwaa</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/fdwacwaa' target=\"_blank\">playful-sweep-4</a></strong> to <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/fdwacwaa' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/fdwacwaa</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>▁▇▄█▁▄▁▄▄▁▁▁▄▁▄▄▁▁▁█▁▁▁▁▁▁▁▁▁▄▁▁▄▁▁▁▁▁▂▁</td></tr><tr><td>grad_norm</td><td>▂▁▁▁▁▁▁▁▁▁▂█▁▁▂▁▁▁▂▁▁▁▁▄▁▁▁▂▁▁▁▁▁▁▁▂▂▁▁▂</td></tr><tr><td>lr</td><td>▁▂▅▆▇████████▇▇▇▇▇▆▆▅▅▅▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>step_time_seconds</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁█▁█▁▁▁▁█▁▁▁▁▁▁▁▁▁▂▂▁</td></tr><tr><td>tokens_per_second</td><td>█▆▃▂▁▁▂▂▂▂▂▂▁▂▁▂▁▂▂▂▁▂▂▁▁▁▂▂▁▂▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>tokens_processed</td><td>▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>███▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>1.6947</td></tr><tr><td>grad_norm</td><td>0.33957</td></tr><tr><td>lr</td><td>8e-05</td></tr><tr><td>step_time_seconds</td><td>0.41715</td></tr><tr><td>tokens_per_second</td><td>72234.29814</td></tr><tr><td>tokens_processed</td><td>40960000</td></tr><tr><td>train_loss</td><td>2.50557</td></tr><tr><td>train_perplexity</td><td>12.25052</td></tr><tr><td>valid_loss</td><td>2.45273</td></tr><tr><td>valid_perplexity</td><td>11.62008</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size_128</strong> at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/fdwacwaa' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/fdwacwaa</a><br> View project at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250723_155943-fdwacwaa/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oyd5jpgn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250723_160927-oyd5jpgn</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/oyd5jpgn' target=\"_blank\">summer-sweep-5</a></strong> to <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/i8mzslpy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/oyd5jpgn' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/oyd5jpgn</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>▅▅▅▅▅▂▂▂▁▁▂▃▆▁█▅▇▂▂▅▃▅▅▅▂▆▆▇▅▂▃▂▇▂▂▂▂▇▂▅</td></tr><tr><td>grad_norm</td><td>▂▃▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁▁▁▁▂▂▂█▁▁▁▁▁▂▁▁▁▁▂▁</td></tr><tr><td>lr</td><td>▅███████▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_time_seconds</td><td>▁▂▂▁▂▂▂█▂▂▂▇▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tokens_per_second</td><td>█▃▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tokens_processed</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>█▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_perplexity</td><td>██▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>3.00647</td></tr><tr><td>grad_norm</td><td>0.44879</td></tr><tr><td>lr</td><td>0.00016</td></tr><tr><td>step_time_seconds</td><td>0.75532</td></tr><tr><td>tokens_per_second</td><td>74893.84578</td></tr><tr><td>tokens_processed</td><td>40960000</td></tr><tr><td>train_loss</td><td>3.28608</td></tr><tr><td>train_perplexity</td><td>26.73779</td></tr><tr><td>valid_loss</td><td>3.28382</td></tr><tr><td>valid_perplexity</td><td>26.67759</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size_256</strong> at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/oyd5jpgn' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/oyd5jpgn</a><br> View project at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250723_160927-oyd5jpgn/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_text(trainer, input_text, max_length, topk=50, temperature=0.5):\n",
    "    # Encode input and add batch dimension\n",
    "    tokens = trainer.tokenizer.encode(input_text)\n",
    "    eot = trainer.tokenizer.encode('<|endoftext|>')\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(trainer.args.device)\n",
    "\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - x.size(1)):  # Limit iterations\n",
    "            # Truncate if exceeds context length\n",
    "            if x.size(1) >= trainer.args.context_length:\n",
    "                x = x[:, -trainer.args.context_length:]\n",
    "\n",
    "            # Get logits and apply temperature\n",
    "            logits = trainer.model(x)[:, -1, :] / temperature\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-k sampling\n",
    "            topk_probs, topk_indices = torch.topk(probs, topk, dim=-1)\n",
    "            ix = torch.multinomial(topk_probs, 1)\n",
    "            next_token = torch.gather(topk_indices, -1, ix)\n",
    "\n",
    "\n",
    "            # Append token\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "            # Optional: Stop if end token is generated\n",
    "            if next_token.item() == eot[0]:\n",
    "              break\n",
    "\n",
    "    # Decode and return\n",
    "    tokens = x[0].tolist()\n",
    "    return trainer.tokenizer.decode(tokens)"
   ],
   "metadata": {
    "id": "_gWml9iRcO2o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "NAHLftGuvLo2"
   }
  }
 ]
}