{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16487,
     "status": "ok",
     "timestamp": 1753481428536,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "zEQPyRsDh2Gx",
    "outputId": "860997f7-d83c-48bc-922c-0770231be1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-25 22:10:12--  https://huggingface.co/datasets/dilawarm/assignment-1/resolve/main/owt_train_tokens.npy\n",
      "Resolving huggingface.co (huggingface.co)... 13.35.37.32, 13.35.37.30, 13.35.37.96, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.35.37.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/687d321cfc3db7d505705eea/6de22bc5c74ecc9d7f3ef7e0f50906003fb831890739ca2ae4b16b8d798da131?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250725T221012Z&X-Amz-Expires=3600&X-Amz-Signature=44fc74f685de1147cf6e5f9219c9881ff56728131e4256e7142cb6720a748106&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_train_tokens.npy%3B+filename%3D%22owt_train_tokens.npy%22%3B&x-id=GetObject&Expires=1753485012&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzQ4NTAxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODdkMzIxY2ZjM2RiN2Q1MDU3MDVlZWEvNmRlMjJiYzVjNzRlY2M5ZDdmM2VmN2UwZjUwOTA2MDAzZmI4MzE4OTA3MzljYTJhZTRiMTZiOGQ3OThkYTEzMSoifV19&Signature=VIiPhI19E4D9u1Ke3pGzQu6QWha8566-mUFHJZxHe1S6Mvjk%7Emo%7E%7E5g3u8F9l5HIao0EnJedLR53JQxgLfslADP6xa1ci4TapTHQZ-JGn7%7Eyxow6N2qn8t1tYouJTx7sbJ1eHG4KbArI6tHDCJujZiqLD%7Esl2BgwpuqJEKgC82eL9-D3gHHCnCvkmufQfw%7EtSslXFSLUWaUcFN1S8Yd75LB48dMlwXl-XCL95gwrsOV2DwtPV%7ExcM35Ev0FhhHHb21WbluZoCxNR%7ESCYOI-%7EZHqWqj5cS-v-PHS%7EFmk2mwstOHqOtzvICwUFP3G-CC1cQ9vfFIUj3q25ZXcCUsV6zw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2025-07-25 22:10:12--  https://cas-bridge.xethub.hf.co/xet-bridge-us/687d321cfc3db7d505705eea/6de22bc5c74ecc9d7f3ef7e0f50906003fb831890739ca2ae4b16b8d798da131?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250725T221012Z&X-Amz-Expires=3600&X-Amz-Signature=44fc74f685de1147cf6e5f9219c9881ff56728131e4256e7142cb6720a748106&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_train_tokens.npy%3B+filename%3D%22owt_train_tokens.npy%22%3B&x-id=GetObject&Expires=1753485012&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzQ4NTAxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODdkMzIxY2ZjM2RiN2Q1MDU3MDVlZWEvNmRlMjJiYzVjNzRlY2M5ZDdmM2VmN2UwZjUwOTA2MDAzZmI4MzE4OTA3MzljYTJhZTRiMTZiOGQ3OThkYTEzMSoifV19&Signature=VIiPhI19E4D9u1Ke3pGzQu6QWha8566-mUFHJZxHe1S6Mvjk%7Emo%7E%7E5g3u8F9l5HIao0EnJedLR53JQxgLfslADP6xa1ci4TapTHQZ-JGn7%7Eyxow6N2qn8t1tYouJTx7sbJ1eHG4KbArI6tHDCJujZiqLD%7Esl2BgwpuqJEKgC82eL9-D3gHHCnCvkmufQfw%7EtSslXFSLUWaUcFN1S8Yd75LB48dMlwXl-XCL95gwrsOV2DwtPV%7ExcM35Ev0FhhHHb21WbluZoCxNR%7ESCYOI-%7EZHqWqj5cS-v-PHS%7EFmk2mwstOHqOtzvICwUFP3G-CC1cQ9vfFIUj3q25ZXcCUsV6zw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.14, 18.155.68.46, 18.155.68.69, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5454352116 (5.1G)\n",
      "Saving to: ‘owt_train_tokens.npy’\n",
      "\n",
      "owt_train_tokens.np 100%[===================>]   5.08G   131MB/s    in 15s     \n",
      "\n",
      "2025-07-25 22:10:28 (338 MB/s) - ‘owt_train_tokens.npy’ saved [5454352116/5454352116]\n",
      "\n",
      "--2025-07-25 22:10:28--  https://huggingface.co/datasets/dilawarm/assignment-1/resolve/main/owt_valid_tokens.npy\n",
      "Resolving huggingface.co (huggingface.co)... 13.35.37.32, 13.35.37.30, 13.35.37.96, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.35.37.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/687d321cfc3db7d505705eea/a0116bbd940323cc328bbb5b5e68a0629045bc7f65679d16c28c724d0666db93?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250725T221028Z&X-Amz-Expires=3600&X-Amz-Signature=1afbc41e80c52a5b140ff2b5b1028e7577954cb2f91d3f7fc1fde0c90d41dadb&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_valid_tokens.npy%3B+filename%3D%22owt_valid_tokens.npy%22%3B&x-id=GetObject&Expires=1753485028&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzQ4NTAyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODdkMzIxY2ZjM2RiN2Q1MDU3MDVlZWEvYTAxMTZiYmQ5NDAzMjNjYzMyOGJiYjViNWU2OGEwNjI5MDQ1YmM3ZjY1Njc5ZDE2YzI4YzcyNGQwNjY2ZGI5MyoifV19&Signature=T%7ENlntjNv-QKXHuLVt9FobdImjWLkGInGDgTKp60o0dKao%7EiA1SO7-a8u51-zsGrWEuHnaTlm77bfOdLffYG64yWqqGpUvXDEl1IKS%7EC0iH79TLtv0JhrFabDMUWyNAz2ArICAnwixgxssabEn2wekQ4Wjca4hfPOu5y7TRQAoDnzIEYoc%7EDXcImWpAdLFeHsulazRsJI3u-XyJp2w1hePJSvUpYzbtmA5OzjjZNoGOU4ZQrm0CUJyf%7EqKCwV9fB88im-f8DY9Bds7MEeXCed26TtiY5JSTXZPTAklEIrMjwW8owaymMMEvp7AiJyjZPx0XxfsisUuedy5z-31Rp1w__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2025-07-25 22:10:28--  https://cas-bridge.xethub.hf.co/xet-bridge-us/687d321cfc3db7d505705eea/a0116bbd940323cc328bbb5b5e68a0629045bc7f65679d16c28c724d0666db93?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250725%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250725T221028Z&X-Amz-Expires=3600&X-Amz-Signature=1afbc41e80c52a5b140ff2b5b1028e7577954cb2f91d3f7fc1fde0c90d41dadb&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_valid_tokens.npy%3B+filename%3D%22owt_valid_tokens.npy%22%3B&x-id=GetObject&Expires=1753485028&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzQ4NTAyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODdkMzIxY2ZjM2RiN2Q1MDU3MDVlZWEvYTAxMTZiYmQ5NDAzMjNjYzMyOGJiYjViNWU2OGEwNjI5MDQ1YmM3ZjY1Njc5ZDE2YzI4YzcyNGQwNjY2ZGI5MyoifV19&Signature=T%7ENlntjNv-QKXHuLVt9FobdImjWLkGInGDgTKp60o0dKao%7EiA1SO7-a8u51-zsGrWEuHnaTlm77bfOdLffYG64yWqqGpUvXDEl1IKS%7EC0iH79TLtv0JhrFabDMUWyNAz2ArICAnwixgxssabEn2wekQ4Wjca4hfPOu5y7TRQAoDnzIEYoc%7EDXcImWpAdLFeHsulazRsJI3u-XyJp2w1hePJSvUpYzbtmA5OzjjZNoGOU4ZQrm0CUJyf%7EqKCwV9fB88im-f8DY9Bds7MEeXCed26TtiY5JSTXZPTAklEIrMjwW8owaymMMEvp7AiJyjZPx0XxfsisUuedy5z-31Rp1w__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.69, 18.155.68.125, 18.155.68.14, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 132805066 (127M)\n",
      "Saving to: ‘owt_valid_tokens.npy’\n",
      "\n",
      "owt_valid_tokens.np 100%[===================>] 126.65M   372MB/s    in 0.3s    \n",
      "\n",
      "2025-07-25 22:10:28 (372 MB/s) - ‘owt_valid_tokens.npy’ saved [132805066/132805066]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/dilawarm/assignment-1/resolve/main/owt_train_tokens.npy\n",
    "!wget https://huggingface.co/datasets/dilawarm/assignment-1/resolve/main/owt_valid_tokens.npy\n",
    "!wget https://huggingface.co/datasets/dilawarm/assignment-1/resolve/main/openwebtext_vocab.json\n",
    "!wget https://huggingface.co/datasets/dilawarm/assignment-1/resolve/main/openwebtext_merges.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10624,
     "status": "ok",
     "timestamp": 1753481439790,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "QTaD2xnoVdnH",
    "outputId": "2974c128-c567-42af-e1a8-c3cdfe06865a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jaxtyping\n",
      "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
      "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: wadler-lindig, jaxtyping\n",
      "Successfully installed jaxtyping-0.3.2 wadler-lindig-0.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip install jaxtyping\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1753481439857,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "OFu3n8ZOZfQv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BPE Tokenizer for encoding and decoding text.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "import regex\n",
    "\n",
    "# GPT-2 pre-tokenization pattern from tiktoken\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "COMPILED_PAT = regex.compile(PAT)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    BPE tokenizer for encoding and decoding text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]], special_tokens: list[str] | None = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with vocabulary, merges, and special tokens.\n",
    "\n",
    "        Args:\n",
    "            vocab: Dictionary mapping token IDs to bytes\n",
    "            merges: List of BPE merges in order of creation\n",
    "            special_tokens: Optional list of special tokens to preserve\n",
    "        \"\"\"\n",
    "        self.vocab = vocab.copy()\n",
    "        self.merges = merges.copy()\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "        self.vocab_reverse: dict[bytes, int] = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "        next_token_id = max(self.vocab.keys()) + 1 if self.vocab else 0\n",
    "        for special_token in self.special_tokens:\n",
    "            special_bytes = special_token.encode(\"utf-8\")\n",
    "            if special_bytes not in self.vocab_reverse:\n",
    "                self.vocab[next_token_id] = special_bytes\n",
    "                self.vocab_reverse[special_bytes] = next_token_id\n",
    "                next_token_id += 1\n",
    "\n",
    "        self.merge_ranks: dict[tuple[bytes, bytes], int] = {merge: i for i, merge in enumerate(self.merges)}\n",
    "\n",
    "        if self.special_tokens:\n",
    "            sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)\n",
    "            escaped_tokens = [re.escape(token) for token in sorted_special_tokens]\n",
    "            self.special_pattern = \"|\".join(escaped_tokens)\n",
    "            self.special_regex = regex.compile(f\"({self.special_pattern})\")\n",
    "        else:\n",
    "            self.special_regex = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(\n",
    "        cls, vocab_filepath: str, merges_filepath: str, special_tokens: list[str] | None = None\n",
    "    ) -> Tokenizer:\n",
    "        \"\"\"\n",
    "        Create a tokenizer from saved vocabulary and merges files.\n",
    "\n",
    "        Args:\n",
    "            vocab_filepath: Path to vocabulary file\n",
    "            merges_filepath: Path to merges file\n",
    "            special_tokens: Optional list of special tokens\n",
    "\n",
    "        Returns:\n",
    "            Initialized Tokenizer instance\n",
    "        \"\"\"\n",
    "        if vocab_filepath.endswith(\".json\"):\n",
    "            with open(vocab_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                vocab_data = json.load(f)\n",
    "                vocab = {}\n",
    "                for k, v in vocab_data.items():\n",
    "                    token_id = int(k)\n",
    "                    if isinstance(v, str):\n",
    "                        try:\n",
    "                            vocab[token_id] = bytes.fromhex(v)\n",
    "                        except ValueError:\n",
    "                            vocab[token_id] = v.encode(\"utf-8\")\n",
    "                    elif isinstance(v, list):\n",
    "                        vocab[token_id] = bytes(v)\n",
    "                    else:\n",
    "                        vocab[token_id] = v\n",
    "        else:\n",
    "            with open(vocab_filepath, \"rb\") as f:\n",
    "                vocab = pickle.load(f)\n",
    "\n",
    "        if merges_filepath.endswith(\".json\"):\n",
    "            with open(merges_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                merges_data = json.load(f)\n",
    "                merges = []\n",
    "                for merge in merges_data:\n",
    "                    if isinstance(merge[0], str):\n",
    "                        merge_tuple = (merge[0].encode(\"utf-8\"), merge[1].encode(\"utf-8\"))\n",
    "                    elif isinstance(merge[0], list):\n",
    "                        merge_tuple = (bytes(merge[0]), bytes(merge[1]))\n",
    "                    else:\n",
    "                        merge_tuple = merge\n",
    "                    merges.append(merge_tuple)\n",
    "        else:\n",
    "            with open(merges_filepath, \"rb\") as f:\n",
    "                merges = pickle.load(f)\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Encode text into a sequence of token IDs.\n",
    "\n",
    "        Args:\n",
    "            text: Input text to encode\n",
    "\n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        if self.special_regex:\n",
    "            text_parts = self.special_regex.split(text)\n",
    "        else:\n",
    "            text_parts = [text]\n",
    "\n",
    "        token_ids: list[int] = []\n",
    "\n",
    "        for part in text_parts:\n",
    "            if part in self.special_tokens:\n",
    "                special_bytes = part.encode(\"utf-8\")\n",
    "                token_ids.append(self.vocab_reverse[special_bytes])\n",
    "            elif part:\n",
    "                part_ids = self._encode_text_part(part)\n",
    "                token_ids.extend(part_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def _encode_text_part(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Encode a text part (not containing special tokens) using BPE.\n",
    "\n",
    "        Args:\n",
    "            text: Text part to encode\n",
    "\n",
    "        Returns:\n",
    "            List of token IDs for this text part\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        token_ids: list[int] = []\n",
    "\n",
    "        for match in COMPILED_PAT.finditer(text):\n",
    "            pre_token = match.group()\n",
    "            if pre_token:\n",
    "                byte_sequence = pre_token.encode(\"utf-8\")\n",
    "                word_tokens = self._apply_bpe(byte_sequence)\n",
    "\n",
    "                for token in word_tokens:\n",
    "                    if token in self.vocab_reverse:\n",
    "                        token_ids.append(self.vocab_reverse[token])\n",
    "                    else:\n",
    "                        for byte_val in token:\n",
    "                            byte_token = bytes([byte_val])\n",
    "                            if byte_token in self.vocab_reverse:\n",
    "                                token_ids.append(self.vocab_reverse[byte_token])\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def _apply_bpe(self, byte_sequence: bytes) -> list[bytes]:\n",
    "        \"\"\"\n",
    "        Apply BPE merges to a byte sequence\n",
    "\n",
    "        Args:\n",
    "            byte_sequence: Input bytes to apply BPE to\n",
    "\n",
    "        Returns:\n",
    "            List of byte tokens after applying BPE merges\n",
    "        \"\"\"\n",
    "        if len(byte_sequence) <= 1:\n",
    "            return [byte_sequence]\n",
    "\n",
    "        word = [bytes([b]) for b in byte_sequence]\n",
    "\n",
    "        while True:\n",
    "            pairs = []\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                if pair in self.merge_ranks:\n",
    "                    pairs.append((self.merge_ranks[pair], i, pair))\n",
    "\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            pairs.sort()\n",
    "            _, _, (first, second) = pairs[0]\n",
    "\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == first and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "\n",
    "            word = new_word\n",
    "\n",
    "        return word\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode a sequence of token IDs back to text.\n",
    "\n",
    "        Args:\n",
    "            ids: List of token IDs to decode\n",
    "\n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        if not ids:\n",
    "            return \"\"\n",
    "\n",
    "        byte_tokens: list[bytes] = []\n",
    "        for token_id in ids:\n",
    "            if token_id in self.vocab:\n",
    "                byte_tokens.append(self.vocab[token_id])\n",
    "\n",
    "        combined_bytes = b\"\".join(byte_tokens)\n",
    "\n",
    "        try:\n",
    "            return combined_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        except Exception:\n",
    "            return combined_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        \"\"\"\n",
    "        Memory-efficient encoding of an iterable of strings.\n",
    "\n",
    "        This method processes the input line by line to minimize memory usage,\n",
    "        making it suitable for large files that don't fit in memory.\n",
    "\n",
    "        Args:\n",
    "            iterable: An iterable of strings (e.g., file handle)\n",
    "\n",
    "        Yields:\n",
    "            Token IDs one at a time\n",
    "        \"\"\"\n",
    "        buffer = \"\"\n",
    "\n",
    "        for line in iterable:\n",
    "            buffer += line\n",
    "\n",
    "            while buffer:\n",
    "                if len(buffer) > 8192:  # Process in 8KB chunks\n",
    "                    # Find last newline in first 8KB\n",
    "                    chunk_end = buffer.rfind(\"\\n\", 0, 8192)\n",
    "                    if chunk_end == -1:\n",
    "                        # No newline found, take a smaller chunk at word boundary\n",
    "                        chunk_end = buffer.rfind(\" \", 0, 4096)\n",
    "                        if chunk_end == -1:\n",
    "                            chunk_end = 4096  # Force split if no word bounary\n",
    "\n",
    "                    chunk = buffer[: chunk_end + 1]\n",
    "                    buffer = buffer[chunk_end + 1 :]\n",
    "\n",
    "                    token_ids = self.encode(chunk)\n",
    "                    yield from token_ids\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if buffer:\n",
    "            token_ids = self.encode(buffer)\n",
    "            yield from token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3832,
     "status": "ok",
     "timestamp": 1753481443695,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "cVpiEH7xVECk"
   },
   "outputs": [],
   "source": [
    "from jaxtyping import Float, Int\n",
    "import numpy.typing as npt\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features:int, out_features:int, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        W = torch.empty(out_features, in_features, device=device, dtype=dtype)\n",
    "        std = (2/(in_features + out_features)**(0.5))\n",
    "        torch.nn.init.trunc_normal_(W, mean=0, std=std, a=-3*std, b=3*std)\n",
    "        self.W = nn.Parameter(W)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return einsum(x, self.W, \"... d_in, d_out d_in -> ... d_out\")\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size:int, d_model:int, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        W = torch.empty(self.vocab_size, self.d_model, device=device, dtype=dtype)\n",
    "        torch.nn.init.trunc_normal_(W, mean=0, std=1, a=-3, b=3)\n",
    "        self.W = nn.Parameter(W)\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.W[token_ids]\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.G = nn.Parameter(torch.ones(d_model, device=device, dtype=dtype))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x_squared = x**2\n",
    "        x_squared_mean = x_squared.mean(-1, keepdim=True)\n",
    "        rms = (x_squared_mean + self.eps)**(0.5)\n",
    "        x_normalized = x / rms\n",
    "        result = einsum(x_normalized, self.G, \"... d_model, d_model -> ... d_model\")\n",
    "        return result.to(in_dtype)\n",
    "\n",
    "class SWIGLU(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, device: torch.device | None = None, dtype: torch.dtype | None =None):\n",
    "        super().__init__()\n",
    "        self.W1 = Linear(d_model, d_ff, device, dtype)\n",
    "        self.W2 = Linear(d_ff, d_model, device, dtype)\n",
    "        self.W3 = Linear(d_model, d_ff, device, dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w1x = self.W1(x)\n",
    "        silux = w1x * torch.sigmoid(w1x)\n",
    "        w3x = self.W3(x)\n",
    "        elew1w3 = silux * w3x\n",
    "        return self.W2(elew1w3)\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
    "        super().__init__()\n",
    "        self.r = torch.zeros(max_seq_len, d_k, d_k, device=device)\n",
    "\n",
    "        for i in range(max_seq_len):\n",
    "            for k in range(d_k//2):\n",
    "                freq = 1.0 / (theta ** (2*k / d_k))\n",
    "                angle = i * freq\n",
    "\n",
    "                cos_val = torch.cos(torch.tensor(angle, device=device))\n",
    "                sin_val = torch.sin(torch.tensor(angle, device=device))\n",
    "\n",
    "                self.r[i, 2*k, 2*k] = cos_val\n",
    "                self.r[i, 2*k, 2*k+1] = -sin_val\n",
    "                self.r[i, 2*k+1, 2*k] = sin_val\n",
    "                self.r[i, 2*k+1, 2*k+1] = cos_val\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        ri_token_pos = self.r[token_positions]\n",
    "        return einsum(x, ri_token_pos, \"... seq d_k_in, ... seq d_k_out d_k_in -> ... seq d_k_out\")\n",
    "\n",
    "\n",
    "def softmax(x: torch.Tensor, dim: int):\n",
    "    max_xi = torch.amax(x, dim=dim, keepdim=True)\n",
    "    x_shifted = x - max_xi\n",
    "    x_exp = torch.exp(x_shifted)\n",
    "    sum_x_exp = torch.sum(x_exp, dim=dim, keepdim=True)\n",
    "    result = x_exp / sum_x_exp\n",
    "    return result\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    Q: Float[Tensor, \" ... queries d_k\"],\n",
    "    K: Float[Tensor, \" ... keys d_k\"],\n",
    "    V: Float[Tensor, \" ... values d_v\"],\n",
    "    mask: Float[Tensor, \" ... queries keys\"] | None = None\n",
    ") -> Float[Tensor, \" ... queries d_v\"]:\n",
    "    wei = einsum(Q, K, \"... queries d_k, ... keys d_k -> ... queries keys\") / (Q.shape[-1] ** 0.5)\n",
    "    if mask is not None:\n",
    "        mask = mask.to(wei.device)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "    wei = softmax(wei, dim=-1)\n",
    "    return einsum(wei, V, \"... queries keys, ... keys d_v -> ... queries d_v\")\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model:int, num_heads:int, device=None, rope=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.Q = Linear(d_model, self.dk * self.num_heads, device=device)\n",
    "        self.K = Linear(d_model, self.dk * self.num_heads, device=device)\n",
    "        self.V = Linear(d_model, self.dk * self.num_heads, device=device)\n",
    "        self.Wo = Linear(self.dk * num_heads, d_model, device=device)\n",
    "        self.rope = rope\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        q,k,v = self.Q(x), self.K(x), self.V(x)\n",
    "        q = rearrange(q, \"... seq (num_heads dk) -> ... num_heads seq dk\", num_heads=self.num_heads)\n",
    "        k = rearrange(k, \"... seq (num_heads dk) -> ... num_heads seq dk\", num_heads=self.num_heads)\n",
    "        v = rearrange(v, \"... seq (num_heads dv) -> ... num_heads seq dv\", num_heads=self.num_heads)\n",
    "\n",
    "        if self.rope != None:\n",
    "            q = self.rope(q, token_positions)\n",
    "            k = self.rope(k, token_positions)\n",
    "\n",
    "        seq = k.shape[-2]\n",
    "        attn = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        attn = rearrange(attn, \"... num_heads seq dv -> ... seq (num_heads dv)\")\n",
    "        return self.Wo(attn)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, device=None, rope=None):\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(d_model, num_heads, device=device, rope=rope)\n",
    "        self.ffn = SWIGLU(d_model, d_ff, device=device)\n",
    "        self.ln1 = RMSNorm(d_model, device=device)\n",
    "        self.ln2 = RMSNorm(d_model, device=device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        if token_positions == None:\n",
    "            token_positions = torch.arange(x.shape[1])\n",
    "        x = x + self.mha(self.ln1(x), token_positions)\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size:int, context_length:int, num_layers:int, d_model:int, num_heads:int, d_ff:int, rope_theta=None, device=None):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(vocab_size, d_model, device)\n",
    "        d_k = d_model // num_heads\n",
    "        self.rope = RotaryPositionalEmbedding(rope_theta, d_k, context_length, device)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(d_model, num_heads, d_ff, device, self.rope) for _ in range(num_layers)])\n",
    "        self.lnf = RMSNorm(d_model, device=device)\n",
    "        self.lm_head = Linear(d_model, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        tok_emb = self.emb(x)\n",
    "        block_out = self.blocks(tok_emb)\n",
    "        norm_out = self.lnf(block_out)\n",
    "        logits = self.lm_head(norm_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1753481443744,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "2hxnNFH-Vj5B"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from collections.abc import Callable, Iterable\n",
    "from pathlib import Path\n",
    "from typing import Any, BinaryIO, IO, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import einsum, rearrange\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cross_entropy(logits, targets):\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        defaults = {\"lr\" : lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                t = state.get(\"t\", 0)\n",
    "                p.data -= lr / math.sqrt(t + 1) * p.grad.data\n",
    "                state[\"t\"] = t + 1\n",
    "        return loss\n",
    "\n",
    "class AdamW(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, weight_decay=0.0, betas=(0.9,0.999), eps = 1e-8):\n",
    "        defaults = {\"lr\" : lr, \"betas\": betas, \"weight_decay\": weight_decay, \"eps\": eps}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            b1,b2 = group[\"betas\"]\n",
    "            eps = group[\"eps\"]\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                t = state.get(\"t\", 1)\n",
    "                m = state.get(\"m\", torch.zeros_like(p.data))\n",
    "                v = state.get(\"v\", torch.zeros_like(p.data))\n",
    "                g = p.grad.data\n",
    "                m = b1 * m + (1-b1) * g\n",
    "                v = b2*v + (1-b2) * g**2\n",
    "                lr_t = lr * math.sqrt((1-b2**t)) / (1 - b1**t)\n",
    "                p.data -= lr_t * m / (torch.sqrt(v) + eps)\n",
    "                p.data -= lr * weight_decay * p.data\n",
    "                state[\"t\"] = t + 1\n",
    "                state[\"m\"] = m\n",
    "                state[\"v\"] = v\n",
    "        return loss\n",
    "\n",
    "def lr_cosine_schedule(\n",
    "    it: int,\n",
    "    max_lr: float,\n",
    "    min_lr: float,\n",
    "    warmup_steps: int,\n",
    "    cosine_cycle_iters: int,\n",
    "):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * it / warmup_steps\n",
    "    # 2) if it > cosine_cycle_iters, return min learning rate\n",
    "    if it >= cosine_cycle_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (cosine_cycle_iters - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "def gradient_clipping(parameters: Iterable[torch.nn.Parameter], max_l2_norm: float, eps=1e-6) -> None:\n",
    "    \"\"\"Given a set of parameters, clip their combined gradients to have l2 norm at most max_l2_norm.\n",
    "\n",
    "    Args:\n",
    "        parameters (Iterable[torch.nn.Parameter]): collection of trainable parameters.\n",
    "        max_l2_norm (float): a positive value containing the maximum l2-norm.\n",
    "\n",
    "    The gradients of the parameters (parameter.grad) should be modified in-place.\n",
    "    \"\"\"\n",
    "    grad_params = [p for p in parameters if p.grad is not None]\n",
    "    l2norm = torch.sqrt(sum([torch.sum(p.grad **2) for p in grad_params]))\n",
    "    if l2norm < max_l2_norm:\n",
    "        return l2norm\n",
    "    for p in grad_params:\n",
    "        p.grad *= (max_l2_norm/ (l2norm + eps))\n",
    "    return l2norm\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    dataset: npt.NDArray, batch_size: int, context_length: int, device: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Pre-allocate numpy arrays\n",
    "    xs = np.zeros((batch_size, context_length), dtype=np.int64)\n",
    "    ys = np.zeros((batch_size, context_length), dtype=np.int64)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        idx = np.random.randint(0, len(dataset) - context_length)\n",
    "        xs[i] = dataset[idx: idx + context_length]\n",
    "        ys[i] = dataset[idx+1: idx + context_length + 1]\n",
    "\n",
    "    # Convert to tensors\n",
    "    xs = torch.from_numpy(xs).to(device)\n",
    "    ys = torch.from_numpy(ys).to(device)\n",
    "    return (xs, ys)\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    iteration: int,\n",
    "    out: str | os.PathLike | BinaryIO | IO[bytes],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a model, optimizer, and an iteration number, serialize them to disk.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Serialize the state of this model.\n",
    "        optimizer (torch.optim.Optimizer): Serialize the state of this optimizer.\n",
    "        iteration (int): Serialize this value, which represents the number of training iterations\n",
    "            we've completed.\n",
    "        out (str | os.PathLike | BinaryIO | IO[bytes]): Path or file-like object to serialize the model, optimizer, and iteration to.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"iteration\": iteration\n",
    "    }\n",
    "    torch.save(checkpoint, out)\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    src: str | os.PathLike | BinaryIO | IO[bytes],\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a serialized checkpoint (path or file-like object), restore the\n",
    "    serialized state to the given model and optimizer.\n",
    "    Return the number of iterations that we previously serialized in\n",
    "    the checkpoint.\n",
    "\n",
    "    Args:\n",
    "        src (str | os.PathLike | BinaryIO | IO[bytes]): Path or file-like object to serialized checkpoint.\n",
    "        model (torch.nn.Module): Restore the state of this model.\n",
    "        optimizer (torch.optim.Optimizer): Restore the state of this optimizer.\n",
    "    Returns:\n",
    "        int: the previously-serialized number of iterations.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(src)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    return checkpoint[\"iteration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "executionInfo": {
     "elapsed": 25314,
     "status": "ok",
     "timestamp": 1753481469064,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "45lioJ092y17",
    "outputId": "8e661a17-ef88-435b-c2a8-be5d0f7539c9"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkl4kennylee81\u001b[0m (\u001b[33mkl4kennylee81-kenneth-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1753481469359,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "ESqKGuiyUMBT"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Literal, cast, Optional, BinaryIO, IO\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Default training configuration at top of file\n",
    "DefaultTrainModelArgs = {\n",
    "    # Model args\n",
    "    \"vocab_size\": 32000,\n",
    "    \"context_length\": 256,\n",
    "    \"num_layers\": 4,\n",
    "    \"d_model\": 512,\n",
    "    \"num_heads\": 16,\n",
    "    \"d_ff\": 1344,\n",
    "    \"rope_theta\": 10000,\n",
    "\n",
    "    # Optimizer args\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "\n",
    "    # Learning rate schedule\n",
    "    \"max_learning_rate\": 2e-3,\n",
    "    \"min_learning_rate\": 1e-5,\n",
    "    \"warmup_iters\": 2000,\n",
    "    \"cosine_cycle_iters\": 40960,\n",
    "\n",
    "    # Data paths - keep as is\n",
    "    \"training_set\": \"owt_train_tokens.npy\",\n",
    "    \"validation_set\": \"owt_valid_tokens.npy\",\n",
    "    \"tokenizer_vocab\": \"openwebtext_vocab.json\",\n",
    "    \"tokenizer_merges\": \"openwebtext_merges.pkl\",\n",
    "\n",
    "    # Training config\n",
    "    \"validation_step_interval\": 500,\n",
    "    \"checkpoint_step_interval\": 10000,\n",
    "    \"steps\": 40960,  # 327M tokens target\n",
    "    \"batch_size\": 32,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "\n",
    "    # gdrive\n",
    "    \"save_gdrive\": False,\n",
    "    \"load_model_gdrive\": \"\",\n",
    "\n",
    "    # Device\n",
    "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "\n",
    "    # wandb\n",
    "    \"wandb_active\": False,\n",
    "    \"wandb_run\": \"\"\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class TrainModelArgs:\n",
    "    # model args\n",
    "    vocab_size: int = 32000\n",
    "    context_length: int = 256\n",
    "    num_layers: int = 4\n",
    "    d_model: int = 512\n",
    "    num_heads: int = 16\n",
    "    d_ff: int = 1344\n",
    "    rope_theta: Optional[int] = 10000\n",
    "\n",
    "    # adamw args\n",
    "    weight_decay: float = 0.01\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "\n",
    "    # Learning rate schedule\n",
    "    max_learning_rate: float = 1e-3\n",
    "    min_learning_rate: float = 1e-5\n",
    "    warmup_iters: int = 2000\n",
    "    cosine_cycle_iters: int = 40960\n",
    "\n",
    "    # training loop args\n",
    "    training_set: str | os.PathLike | BinaryIO | IO[bytes] = \"owt_train_tokens.npy\"\n",
    "    validation_set: str | os.PathLike | BinaryIO | IO[bytes] = \"owt_valid_tokens.npy\"\n",
    "    tokenizer_vocab: str | os.PathLike | BinaryIO | IO[bytes] = \"openwebtext_vocab.json\"\n",
    "    tokenizer_merges: str | os.PathLike | BinaryIO | IO[bytes] = \"openwebtext_merges.pkl\"\n",
    "\n",
    "    validation_step_interval: int = 500\n",
    "    checkpoint_step_interval: int = 10000\n",
    "    steps: int = 40960\n",
    "    batch_size: int = 32\n",
    "    gradient_clipping: Optional[float] = 1.0\n",
    "\n",
    "    # wandb logging\n",
    "    wandb_active: bool = False\n",
    "    wandb_run: Optional[str] = \"\"\n",
    "\n",
    "    # device\n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self, args: TrainModelArgs):\n",
    "        self.args = args\n",
    "        self.cur_step = 0\n",
    "        self.model = Transformer(\n",
    "            vocab_size=args.vocab_size,\n",
    "            context_length=args.context_length,\n",
    "            num_layers=args.num_layers,\n",
    "            num_heads=args.num_heads,\n",
    "            d_model=args.d_model,\n",
    "            d_ff=args.d_ff,\n",
    "            rope_theta=args.rope_theta,\n",
    "            device=args.device\n",
    "        )\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.max_learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            betas=args.betas\n",
    "        )\n",
    "\n",
    "        self.tokenizer = Tokenizer.from_files(args.tokenizer_vocab, args.tokenizer_merges, [\"<|endoftext|>\"])\n",
    "\n",
    "        self.training_set = np.load(self.args.training_set, mmap_mode='r')\n",
    "        self.validation_set = np.load(self.args.validation_set, mmap_mode='r')\n",
    "\n",
    "        if args.wandb_active and wandb.run:\n",
    "            wandb.watch(self.model, log=cast(Literal[\"gradients\", \"parameters\", \"all\"], \"gradients\"), log_freq=10)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            total_size = self.training_set.size + self.validation_set.size\n",
    "            eval_size = total_size // 1000\n",
    "            num_batches = eval_size // (self.args.batch_size * self.args.context_length)\n",
    "\n",
    "            num_batches = max(1, num_batches)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                x, label = get_batch(self.validation_set, self.args.batch_size, self.args.context_length, device=self.args.device)\n",
    "                with torch.autocast(device_type=self.args.device.type, dtype=torch.bfloat16):\n",
    "                  output = self.model(x)\n",
    "                loss = cross_entropy(output, label)\n",
    "                loss = cross_entropy(output, label)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = torch.tensor(total_loss / num_batches)\n",
    "            perplexity = avg_loss.exp()\n",
    "            return avg_loss, perplexity\n",
    "\n",
    "    def train(self):\n",
    "        valid_loss, valid_perplexity = self.evaluate()\n",
    "        if self.args.wandb_active and wandb.run:\n",
    "            wandb.log({\"valid_loss\": valid_loss, \"valid_perplexity\": valid_perplexity}, step=self.cur_step)\n",
    "\n",
    "        pbar = tqdm(range(self.cur_step, self.args.steps))\n",
    "        start_time = time.time()\n",
    "        tokens_processed = 0\n",
    "\n",
    "        for step in pbar:\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            self.cur_step = step\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            lr = lr_cosine_schedule(\n",
    "                step,\n",
    "                self.args.max_learning_rate,\n",
    "                self.args.min_learning_rate,\n",
    "                self.args.warmup_iters,\n",
    "                self.args.cosine_cycle_iters)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            x, targets = get_batch(self.training_set, self.args.batch_size, self.args.context_length, device=self.args.device)\n",
    "            with torch.autocast(device_type=self.args.device.type, dtype=torch.bfloat16):\n",
    "              logits = self.model(x)\n",
    "            loss = cross_entropy(logits, targets)\n",
    "            loss.backward()\n",
    "            l2norm = gradient_clipping(self.model.parameters(), self.args.gradient_clipping)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Calculate metrics\n",
    "            batch_tokens = x.shape[0] * x.shape[1]\n",
    "            tokens_processed += batch_tokens\n",
    "            elapsed_time = time.time() - start_time\n",
    "            tokens_per_second = tokens_processed / elapsed_time if elapsed_time > 0 else 0\n",
    "            dt = time.time() - step_start_time\n",
    "            if (step % self.args.validation_step_interval == 0 and step > 0) or (step == self.args.steps-1):\n",
    "                valid_loss, valid_perplexity = self.evaluate()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.2f}\",\n",
    "                \"valid_loss\": f\"{valid_loss.item():.2f}\",\n",
    "                \"valid_perplexity\": f\"{valid_perplexity.item():.2f}\",\n",
    "            })\n",
    "\n",
    "            if self.args.wandb_active and wandb.run:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": loss.item(),\n",
    "                    \"train_perplexity\": loss.exp().item(),\n",
    "                    \"valid_loss\": valid_loss.item(),\n",
    "                    \"valid_perplexity\": valid_perplexity.item(),\n",
    "                    \"grad_norm\": l2norm,\n",
    "                    \"lr\": lr,\n",
    "                    \"tokens_per_second\": tokens_per_second,\n",
    "                    \"step_time_seconds\": dt,\n",
    "                    \"gpu_memory_gb\": torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0,\n",
    "                    \"tokens_processed\": tokens_processed,\n",
    "                }, step=step)\n",
    "\n",
    "        if self.args.wandb_active and wandb.run:\n",
    "            local_checkpoint_path = f'{self.args.wandb_run}-checkpoint-{step}.pth'\n",
    "            save_checkpoint(self.model, self.optimizer, step, local_checkpoint_path)\n",
    "\n",
    "            artifact = wandb.Artifact(f\"{self.args.wandb_run}-checkpoint_{step}\", type=\"model\")\n",
    "            artifact.add_file(local_checkpoint_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "            # Clean up local file after uploading to wandb\n",
    "            os.remove(local_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "91eb8f40154d411d818159917e5c77cd",
      "f6927ac905ea4ebe9538d5e72894aef7",
      "d8a2583800b44fb48e9fb1b2514f8016",
      "46d22876ec7f4ff59d863dbef75327a5",
      "4a7e31b27902433baeed50d33096bc2f",
      "23b07b2b1c33463390e6853cfe3c4fce",
      "aaa38f8da5474f6c8b95f5292f4d81b2",
      "ee29701d68c949fd8a20d5966dde25f9",
      "58a08d26ded04ee69b1429d76a120710",
      "7bfefaef901640ab92b258a5553bba74",
      "70d993c85be740e5a36f664c9c6cb547"
     ]
    },
    "executionInfo": {
     "elapsed": 319472,
     "status": "ok",
     "timestamp": 1753481919789,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "DDk0vplIU9xi",
    "outputId": "f1b42176-0449-49c6-a9f6-3c13f55a439a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: pupezevy\n",
      "Sweep URL: https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/pupezevy\n",
      "Sweep created successfully!\n",
      "Sweep ID: pupezevy\n",
      "Project: cs336-llm-assignment1\n",
      "wandb agent pupezevy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jgoh5unm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250725_221323-jgoh5unm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/jgoh5unm' target=\"_blank\">desert-sweep-1</a></strong> to <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/pupezevy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/pupezevy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/pupezevy' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/sweeps/pupezevy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/jgoh5unm' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/jgoh5unm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91eb8f40154d411d818159917e5c77cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>▅▆▆▆▇▆▅█▆▅▅▇▃▅▂▃▂▅▅▂▂█▄▃▃▃▅▅▆▅▅▃▅█▆▆▁▆▅▂</td></tr><tr><td>grad_norm</td><td>▅▃▄█▂▂▃▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>▁▂▆███████▇▇▇▇▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>step_time_seconds</td><td>▁▁█▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▂▁█▁▁▁▁█▁▁▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>tokens_per_second</td><td>▃▇▃▄█▇▂▅▅▂▅▁▄▅▄▂▁▃▃▃▁▂▂▂▂▂▁▂▁▁▂▂▂▁▂▁▁▂▂▁</td></tr><tr><td>tokens_processed</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_memory_gb</td><td>1.27541</td></tr><tr><td>grad_norm</td><td>0.29302</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>step_time_seconds</td><td>0.06047</td></tr><tr><td>tokens_per_second</td><td>150255.623</td></tr><tr><td>tokens_processed</td><td>40960000</td></tr><tr><td>train_loss</td><td>4.625</td></tr><tr><td>train_perplexity</td><td>102</td></tr><tr><td>valid_loss</td><td>4.74652</td></tr><tr><td>valid_perplexity</td><td>115.18248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">owt-test</strong> at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/jgoh5unm' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1/runs/jgoh5unm</a><br> View project at: <a href='https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1' target=\"_blank\">https://wandb.ai/kl4kennylee81-kenneth-personal/cs336-llm-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250725_221323-jgoh5unm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Complete sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'valid_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [32]},\n",
    "    }\n",
    "}\n",
    "\n",
    "def train_sweep():\n",
    "    # Initialize wandb run\n",
    "    run = wandb.init(\n",
    "      group=\"owt\",\n",
    "      force=True\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    # Create training arguments - only override what's different from defaults\n",
    "    train_args = {\n",
    "        **DefaultTrainModelArgs,\n",
    "\n",
    "        # Only the sweep parameters that differ from DefaultTrainModelArgs\n",
    "        \"steps\": 5000, # 40,960,000 tokens processed\n",
    "        \"batch_size\": config.batch_size,\n",
    "        \"validation_step_interval\": 100,\n",
    "        \"cosine_cycle_iters\": 5000,\n",
    "        \"warmup_iters\": 250,\n",
    "\n",
    "        # wandb settings - always override for sweep\n",
    "        \"wandb_active\": True,\n",
    "        \"wandb_run\" : \"owt-test\"\n",
    "    }\n",
    "\n",
    "    # Initialize and run training\n",
    "    trainer = TrainModel(TrainModelArgs(**train_args))\n",
    "    trainer.model = torch.compile(trainer.model)\n",
    "    config.update(asdict(trainer.args))\n",
    "    wandb.run.name = trainer.args.wandb_run\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "\n",
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=sweep_config,\n",
    "    project=\"cs336-llm-assignment1\",\n",
    "    entity=\"kl4kennylee81-kenneth-personal\"\n",
    ")\n",
    "\n",
    "print(f\"Sweep created successfully!\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "print(f\"Project: cs336-llm-assignment1\")\n",
    "print(f\"wandb agent {sweep_id}\")\n",
    "\n",
    "# Run the sweep agent\n",
    "wandb.agent(sweep_id, train_sweep, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1753482106849,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "kyEqjKIrqs8X",
    "outputId": "ca107987-bc9e-44a7-b755-091a155a59c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66402469\n",
      "2727175994\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainModel(TrainModelArgs(**DefaultTrainModelArgs))\n",
    "print(trainer.validation_set.size)\n",
    "print(trainer.training_set.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1753481532197,
     "user": {
      "displayName": "Kenneth Lee",
      "userId": "12817523688040287060"
     },
     "user_tz": 240
    },
    "id": "_gWml9iRcO2o"
   },
   "outputs": [],
   "source": [
    "def generate_text(trainer, input_text, max_length, topk=50, temperature=0.5):\n",
    "    # Encode input and add batch dimension\n",
    "    tokens = trainer.tokenizer.encode(input_text)\n",
    "    eot = trainer.tokenizer.encode('<|endoftext|>')\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(trainer.args.device)\n",
    "\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - x.size(1)):  # Limit iterations\n",
    "            # Truncate if exceeds context length\n",
    "            if x.size(1) >= trainer.args.context_length:\n",
    "                x = x[:, -trainer.args.context_length:]\n",
    "\n",
    "            # Get logits and apply temperature\n",
    "            logits = trainer.model(x)[:, -1, :] / temperature\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-k sampling\n",
    "            topk_probs, topk_indices = torch.topk(probs, topk, dim=-1)\n",
    "            ix = torch.multinomial(topk_probs, 1)\n",
    "            next_token = torch.gather(topk_indices, -1, ix)\n",
    "\n",
    "\n",
    "            # Append token\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "            # Optional: Stop if end token is generated\n",
    "            if next_token.item() == eot[0]:\n",
    "              break\n",
    "\n",
    "    # Decode and return\n",
    "    tokens = x[0].tolist()\n",
    "    return trainer.tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAHLftGuvLo2"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOk3FN1H7wNC+Mb+2f9UA8Y",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21f572dd1a0f44248b37169935bd25d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b07b2b1c33463390e6853cfe3c4fce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26cd7d3aef3c486e8037f33f2cbe92b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d5b11a9edbe47da8a9a35c7504088fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46d22876ec7f4ff59d863dbef75327a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bfefaef901640ab92b258a5553bba74",
      "placeholder": "​",
      "style": "IPY_MODEL_70d993c85be740e5a36f664c9c6cb547",
      "value": " 5000/5000 [04:34&lt;00:00, 33.72it/s, loss=4.62, valid_loss=4.75, valid_perplexity=115.18]"
     }
    },
    "481d35001ede4f9c913269c4b03923c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a7e31b27902433baeed50d33096bc2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58a08d26ded04ee69b1429d76a120710": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "611aaaa80eec4e6393bd72fe7deb1d10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70d993c85be740e5a36f664c9c6cb547": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79bd8280831e4b64a8129ca364c2bbd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bfefaef901640ab92b258a5553bba74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c67fe66c2b54e748c289b59037693cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82ac1ea4c18b4ee1bf4a83fae91f8615": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d5b11a9edbe47da8a9a35c7504088fa",
      "placeholder": "​",
      "style": "IPY_MODEL_79bd8280831e4b64a8129ca364c2bbd1",
      "value": " 100/100 [00:08&lt;00:00,  2.27it/s, loss=7.06, valid_loss=7.05, valid_perplexity=1147.42]"
     }
    },
    "91eb8f40154d411d818159917e5c77cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6927ac905ea4ebe9538d5e72894aef7",
       "IPY_MODEL_d8a2583800b44fb48e9fb1b2514f8016",
       "IPY_MODEL_46d22876ec7f4ff59d863dbef75327a5"
      ],
      "layout": "IPY_MODEL_4a7e31b27902433baeed50d33096bc2f"
     }
    },
    "92aa1f03c3b24c39b680b6ca334d0564": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26cd7d3aef3c486e8037f33f2cbe92b8",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_481d35001ede4f9c913269c4b03923c3",
      "value": 100
     }
    },
    "938d923da4714322b106fe5b47ae668f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_611aaaa80eec4e6393bd72fe7deb1d10",
      "placeholder": "​",
      "style": "IPY_MODEL_7c67fe66c2b54e748c289b59037693cc",
      "value": "100%"
     }
    },
    "9e22793530fa4a86b1928100ce575e3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_938d923da4714322b106fe5b47ae668f",
       "IPY_MODEL_92aa1f03c3b24c39b680b6ca334d0564",
       "IPY_MODEL_82ac1ea4c18b4ee1bf4a83fae91f8615"
      ],
      "layout": "IPY_MODEL_21f572dd1a0f44248b37169935bd25d4"
     }
    },
    "aaa38f8da5474f6c8b95f5292f4d81b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8a2583800b44fb48e9fb1b2514f8016": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee29701d68c949fd8a20d5966dde25f9",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_58a08d26ded04ee69b1429d76a120710",
      "value": 5000
     }
    },
    "ee29701d68c949fd8a20d5966dde25f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6927ac905ea4ebe9538d5e72894aef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b07b2b1c33463390e6853cfe3c4fce",
      "placeholder": "​",
      "style": "IPY_MODEL_aaa38f8da5474f6c8b95f5292f4d81b2",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
